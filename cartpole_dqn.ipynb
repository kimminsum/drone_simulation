{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "_BaseOptimizer.minimize() missing 1 required positional argument: 'var_list'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Administrator\\Projects\\drone_simulation\\cartpole_dqn.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Projects/drone_simulation/cartpole_dqn.ipynb#X33sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Stochastic Gradient Descent Optimizer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Projects/drone_simulation/cartpole_dqn.ipynb#X33sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mSGD(learning_rate\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Projects/drone_simulation/cartpole_dqn.ipynb#X33sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m train_step \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49mminimize(cost)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Projects/drone_simulation/cartpole_dqn.ipynb#X33sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Helper function: Chooses a random value between the two boundaries.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Projects/drone_simulation/cartpole_dqn.ipynb#X33sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandf\u001b[39m(s, e):\n",
            "\u001b[1;31mTypeError\u001b[0m: _BaseOptimizer.minimize() missing 1 required positional argument: 'var_list'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "\n",
        "# Parameters\n",
        "epsilon = 1\n",
        "epsilon_minimum_value = 0.001\n",
        "nb_actions = 3\n",
        "epoch = 1001\n",
        "hidden_size = 100\n",
        "max_memory = 500\n",
        "batch_size = 50\n",
        "grid_size = 10\n",
        "nb_states = grid_size * grid_size\n",
        "discount = 0.9\n",
        "learning_rate = 0.2\n",
        "\n",
        "# Create the base model.\n",
        "X = tf.keras.Input(shape=(nb_states,), dtype=tf.float32)\n",
        "W1 = tf.Variable(tf.keras.initializers.GlorotNormal()(shape=(nb_states, hidden_size)))\n",
        "b1 = tf.Variable(tf.random.normal([hidden_size], stddev=0.01))\n",
        "input_layer = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.Variable(tf.random.normal([hidden_size, hidden_size], stddev=0.01))\n",
        "b2 = tf.Variable(tf.random.normal([hidden_size], stddev=0.01))\n",
        "hidden_layer = tf.nn.relu(tf.matmul(input_layer, W2) + b2)\n",
        "\n",
        "W3 = tf.Variable(tf.keras.initializers.GlorotNormal()(shape=(hidden_size, nb_actions)))\n",
        "b3 = tf.Variable(tf.random.normal([nb_actions], stddev=0.01))\n",
        "output_layer = tf.matmul(hidden_layer, W3) + b3\n",
        "\n",
        "# True labels\n",
        "Y = tf.keras.Input(shape=(nb_actions,), dtype=tf.float32)\n",
        "\n",
        "# Mean squared error cost function\n",
        "cost = tf.reduce_sum(tf.square(Y - output_layer)) / (2 * batch_size)\n",
        "\n",
        "# Stochastic Gradient Descent Optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "train_step = optimizer.minimize(cost, var_list=[W1, b1, W2, b2, W3, b3])\n",
        "\n",
        "\n",
        "\n",
        "# Helper function: Chooses a random value between the two boundaries.\n",
        "def randf(s, e):\n",
        "    return (float(random.randrange(0, int((e - s) * 9999))) / 10000) + s\n",
        "\n",
        "\n",
        "# The environment: Handles interactions and contains the state of the environment\n",
        "class CatchEnvironment:\n",
        "    def __init__(self, grid_size):\n",
        "        self.grid_size = grid_size\n",
        "        self.nb_states = self.grid_size * self.grid_size\n",
        "        self.state = np.empty(3, dtype=np.uint8)\n",
        "\n",
        "    def observe(self):\n",
        "        canvas = self.draw_state()\n",
        "        canvas = np.reshape(canvas, (-1, self.nb_states))\n",
        "        return canvas\n",
        "\n",
        "    def draw_state(self):\n",
        "        canvas = np.zeros((self.grid_size, self.grid_size))\n",
        "        canvas[self.state[0] - 1, self.state[1] - 1] = 1  # Draw the fruit.\n",
        "        canvas[self.grid_size - 1, self.state[2] - 1 - 1] = 1  # Draw the basket.\n",
        "        canvas[self.grid_size - 1, self.state[2] - 1] = 1\n",
        "        canvas[self.grid_size - 1, self.state[2] - 1 + 1] = 1\n",
        "        return canvas\n",
        "\n",
        "    def reset(self):\n",
        "        initial_fruit_column = random.randrange(1, self.grid_size + 1)\n",
        "        initial_bucket_position = random.randrange(2, self.grid_size + 1 - 1)\n",
        "        self.state = np.array([1, initial_fruit_column, initial_bucket_position])\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        state_info = self.state\n",
        "        fruit_row, fruit_col, basket = state_info\n",
        "        return fruit_row, fruit_col, basket\n",
        "\n",
        "    def get_reward(self):\n",
        "        fruit_row, fruit_col, basket = self.get_state()\n",
        "        if fruit_row == self.grid_size - 1:\n",
        "            return 1 if abs(fruit_col - basket) <= 1 else -1\n",
        "        return 0\n",
        "\n",
        "    def is_game_over(self):\n",
        "        return self.state[0] == self.grid_size - 1\n",
        "\n",
        "    def update_state(self, action):\n",
        "        if action == 1:\n",
        "            action = -1\n",
        "        elif action == 2:\n",
        "            action = 0\n",
        "        else:\n",
        "            action = 1\n",
        "        fruit_row, fruit_col, basket = self.get_state()\n",
        "        new_basket = min(max(2, basket + action), self.grid_size - 1)\n",
        "        fruit_row = fruit_row + 1\n",
        "        self.state = np.array([fruit_row, fruit_col, new_basket])\n",
        "\n",
        "    def act(self, action):\n",
        "        self.update_state(action)\n",
        "        reward = self.get_reward()\n",
        "        game_over = self.is_game_over()\n",
        "        return self.observe(), reward, game_over, self.get_state()\n",
        "\n",
        "\n",
        "# The memory: Handles the internal memory that we add experiences that occur based on agent's actions,\n",
        "# and creates batches of experiences based on the mini-batch size for training.\n",
        "class ReplayMemory:\n",
        "    def __init__(self, grid_size, max_memory, discount):\n",
        "        self.max_memory = max_memory\n",
        "        self.grid_size = grid_size\n",
        "        self.nb_states = self.grid_size * self.grid_size\n",
        "        self.discount = discount\n",
        "        canvas = np.zeros((self.grid_size, self.grid_size))\n",
        "        canvas = np.reshape(canvas, (-1, self.nb_states))\n",
        "        self.input_state = np.empty((self.max_memory, 100), dtype=np.float32)\n",
        "        self.actions = np.zeros(self.max_memory, dtype=np.uint8)\n",
        "        self.next_state = np.empty((self.max_memory, 100), dtype=np.float32)\n",
        "        self.game_over = np.empty(self.max_memory, dtype=np.bool)\n",
        "        self.rewards = np.empty(self.max_memory, dtype=np.int8)\n",
        "        self.count = 0\n",
        "        self.current = 0\n",
        "\n",
        "    def remember(self, current_state, action, reward, next_state, game_over):\n",
        "        self.actions[self.current] = action\n",
        "        self.rewards[self.current] = reward\n",
        "        self.input_state[self.current, ...] = current_state\n",
        "        self.next_state[self.current, ...] = next_state\n",
        "        self.game_over[self.current] = game_over\n",
        "        self.count = max(self.count, self.current + 1)\n",
        "        self.current = (self.current + 1) % self.max_memory\n",
        "\n",
        "    def get_batch(self, model, batch_size, nb_actions, nb_states, sess, X):\n",
        "\n",
        "        memory_length = self.count\n",
        "        chosen_batch_size = min(batch_size, memory_length)\n",
        "\n",
        "        inputs = np.zeros((chosen_batch_size, nb_states))\n",
        "        targets = np.zeros((chosen_batch_size, nb_actions))\n",
        "\n",
        "        for i in range(chosen_batch_size):\n",
        "            if memory_length == 1:\n",
        "                memory_length = 2\n",
        "            random_index = random.randrange(1, memory_length)\n",
        "            current_input_state = np.reshape(self.input_state[random_index], (1, 100))\n",
        "\n",
        "            target = sess.run(model, feed_dict={X: current_input_state})\n",
        "\n",
        "            current_next_state = np.reshape(self.next_state[random_index], (1, 100))\n",
        "            current_outputs = sess.run(model, feed_dict={X: current_next_state})\n",
        "\n",
        "            next_state_max_q = np.amax(current_outputs)\n",
        "            if self.game_over[random_index] == True:\n",
        "                target[0, [self.actions[random_index] - 1]] = self.rewards[random_index]\n",
        "            else:\n",
        "                target[0, [self.actions[random_index] - 1]] = (\n",
        "                        self.rewards[random_index] + self.discount * next_state_max_q\n",
        "                )\n",
        "\n",
        "            inputs[i] = current_input_state\n",
        "            targets[i] = target\n",
        "\n",
        "        return inputs, targets\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    print(\"Training new model\")\n",
        "\n",
        "    # Define Environment\n",
        "    env = CatchEnvironment(grid_size)\n",
        "\n",
        "    # Define Replay Memory\n",
        "    memory = ReplayMemory(grid_size, max_memory, discount)\n",
        "\n",
        "    # Add ops to save and restore all the variables.\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    win_count = 0\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        tf.compat.v1.initialize_all_variables().run()\n",
        "\n",
        "        for i in range(epoch):\n",
        "            err = 0\n",
        "            env.reset()\n",
        "            is_game_over = False\n",
        "            current_state = env.observe()\n",
        "\n",
        "            while not is_game_over:\n",
        "                action = -9999\n",
        "                if randf(0, 1) <= epsilon:\n",
        "                    action = random.randrange(1, nb_actions + 1)\n",
        "                else:\n",
        "                    q = sess.run(output_layer, feed_dict={X: current_state})\n",
        "                    index = q.argmax()\n",
        "                    action = index + 1\n",
        "\n",
        "                if epsilon > epsilon_minimum_value:\n",
        "                    epsilon = epsilon * 0.999\n",
        "\n",
        "                next_state, reward, is_game_over, state_info = env.act(action)\n",
        "\n",
        "                if reward == 1:\n",
        "                    win_count += 1\n",
        "\n",
        "                memory.remember(current_state, action, reward, next_state, is_game_over)\n",
        "\n",
        "                current_state = next_state\n",
        "\n",
        "                inputs, targets = memory.get_batch(output_layer, batch_size, nb_actions, nb_states, sess, X)\n",
        "                _, loss = sess.run([train_step, cost], feed_dict={X: inputs, Y: targets})\n",
        "                err += loss\n",
        "\n",
        "            print(\n",
        "                \"Epoch {}: err = {}: Win count = {} Win ratio = {}\".format(\n",
        "                    i, err, win_count, float(win_count) / float(i + 1) * 100\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Save the variables to disk.\n",
        "        save_path = saver.save(sess, os.getcwd() + \"/model.ckpt\")\n",
        "        print(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tf.compat.v1.app.run()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cartpole_dqn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
